{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4fc7976-9a0a-408e-b286-3fb663317120",
   "metadata": {},
   "source": [
    "# **Word Embedding Layers for Deep Learning with Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94243ae3-6da6-48f5-9c86-6918f70ba098",
   "metadata": {},
   "source": [
    "## **Keras Embedding Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b70dd1-44f7-4130-9f2b-91ab4cf1f9b5",
   "metadata": {},
   "source": [
    "We'll examine how to learn a word embedding while applying a neural network to a text classification task in this part.\n",
    "\n",
    "We'll define a simple problem in which there are 10 text documents, each of which contains a comment on a piece of work that a student contributed. Each text file is assigned a positive \"1\" or a negative \"0\" classification. This is a simple sentiment analysis problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e76923-bb95-4dad-a7a1-ea4201bb4674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef1c910-cd55-40da-8d9d-ddcc99034bf2",
   "metadata": {},
   "source": [
    "### **Step 1** \n",
    "1st we'll start by defining the documents and their respective class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15fd5493-d869-4888-b7ec-b17caa5eeec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Well done!','Good work','Great effort',\n",
    "        'nice work','Excellent!','Weak','Poor effort!',\n",
    "        'not good','poor work','Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b374db81-1cfe-46c3-96ab-45ea6be0e493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Well done!',\n",
       " 'Good work',\n",
       " 'Great effort',\n",
       " 'nice work',\n",
       " 'Excellent!',\n",
       " 'Weak',\n",
       " 'Poor effort!',\n",
       " 'not good',\n",
       " 'poor work',\n",
       " 'Could have done better.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c80aaa8-5471-4756-8541-81a234cad661",
   "metadata": {},
   "source": [
    "## **Step 2**\n",
    "We can then integer encode every document. As a result, integer sequences will be provided to the embedding layer as input.\n",
    "\n",
    "The **one hot()** method in Keras generates a hash representation of each word as an effective integer encoding. We will assume a **vocabulary size** of 100, which is significantly more than is required to lower the likelihood of collisions resulting from the hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541a1988-d36e-4977-9062-3270ca7822fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we discuss above we will first define a vocabulary size\n",
    "vocabSize = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5063f37-498d-4fd2-8272-97635aeb9f01",
   "metadata": {},
   "source": [
    "#### Integer encoding documents using one_hot representation.\n",
    "one_hote require text doc which we want to convert and vocabulary size and return the index from the dictionary\n",
    "* **input_text**: Input text (string).\n",
    "* **n**: int. Size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ca141df-9503-4f0a-9553-324ef396f2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[71, 70],\n",
       " [86, 55],\n",
       " [56, 77],\n",
       " [81, 55],\n",
       " [15],\n",
       " [62],\n",
       " [50, 77],\n",
       " [38, 86],\n",
       " [50, 55],\n",
       " [29, 77, 70, 81]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying one_hot\n",
    "oneHotRepresentation = [one_hot(input_text = document, n = vocabSize) for document in corpus]\n",
    "\n",
    "# displaying representation\n",
    "oneHotRepresentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29e60d-781e-425e-8a6e-cd9d8777fb58",
   "metadata": {},
   "source": [
    "## **Defining Embedding layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb1bb9-1acf-4e17-a625-c79d30d73814",
   "metadata": {},
   "source": [
    "Before passing the vectors to keras embedding layer note it down that the lengths of the sequences vary, while Keras prefers vectorized inputs with uniform lengths. In our case all the input sequences will be padded to a **length of 10**. In this case, the **pad sequences()** function from the built-in Keras library can be used to accomplish this.\n",
    "\n",
    "**Args:**\n",
    "* **sequences**: List of sequences (each sequence is a list of integers).\n",
    "* **maxlen**: Optional Int, maximum length of all sequences. If not provided,\n",
    "        sequences will be padded to the length of the longest individual\n",
    "        sequence.\n",
    "* dtype: (Optional, defaults to int32). Type of the output sequences.\n",
    "        To pad sequences with variable length strings, you can use `object`.\n",
    "* **padding**: String, 'pre' or 'post' (optional, defaults to 'pre'):\n",
    "        pad either before or after each sequence.\n",
    "* truncating: String, 'pre' or 'post' (optional, defaults to 'pre'):\n",
    "        remove values from sequences larger than\n",
    "        `maxlen`, either at the beginning or at the end of the sequences.\n",
    "* value: Float or String, padding value. (Optional, defaults to 0.)\n",
    "\n",
    "**Returns**:\n",
    "    Numpy array with shape `(len(sequences), maxlen)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb71011-7f16-4324-a8d3-bf5b5dc7ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pad sequences from keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b4177e-6a98-4f83-9a59-291d157e3495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0 71 70]\n",
      " [ 0  0  0  0  0  0  0  0 86 55]\n",
      " [ 0  0  0  0  0  0  0  0 56 77]\n",
      " [ 0  0  0  0  0  0  0  0 81 55]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 62]\n",
      " [ 0  0  0  0  0  0  0  0 50 77]\n",
      " [ 0  0  0  0  0  0  0  0 38 86]\n",
      " [ 0  0  0  0  0  0  0  0 50 55]\n",
      " [ 0  0  0  0  0  0 29 77 70 81]]\n"
     ]
    }
   ],
   "source": [
    "sentLen = 10\n",
    "paddedCorpus = pad_sequences(sequences = oneHotRepresentation, maxlen = sentLen, padding='pre')\n",
    "print(paddedCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ab27f-e948-40ef-88d0-b66955e0257e",
   "metadata": {},
   "source": [
    "We are now prepared to specify our neural network model's embedding layer.\n",
    "\n",
    "The Embedding has a **100-word vocabulary** and a **10 input length**. We'll pick an embedding space with only **8 dimensions**.\n",
    "\n",
    "### **Embedding Layer Important Arguments**\n",
    "* **input_dim**: Integer. Size of the vocabulary,\n",
    "    i.e. maximum integer index + 1.\n",
    "* **output_dim**: Integer. Dimension of the dense embedding.\n",
    "* **input_length**: Length of input sequences, when it is constant.\n",
    "    This argument is required if you are going to connect\n",
    "    `Flatten` then `Dense` layers upstream\n",
    "    (without it, the shape of the dense outputs cannot be computed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a6a9d9-b477-4a85-86e0-ff93b2e51774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing embidding layer from keras\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "# import sequentional model\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cef90c7-9b99-4b4a-bc52-b589150dfc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 22:13:16.980170: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-20 22:13:16.981744: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# no of features\n",
    "outDim = 8\n",
    "\n",
    "# creating sequential model\n",
    "model = Sequential()\n",
    "# adding embedding layer\n",
    "model.add(Embedding(input_dim = vocabSize, output_dim = outDim, input_length=sentLen))\n",
    "# converting it in 1D\n",
    "model.add(Flatten())\n",
    "# adding sigmoid to classify the input\n",
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd8fe333-6d80-4678-b173-b6ef4d9f7a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 8)             800       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 80)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 881\n",
      "Trainable params: 881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# displaying the summary of our model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95933c3b-c84c-4ba3-b505-49275fc9310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling our model\n",
    "model.compile(optimizer = \"adam\", loss=\"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae98b17-c2fa-4323-a58e-1d5eda593180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 1s 595ms/step - loss: 0.6944 - accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6925 - accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6916 - accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6907 - accuracy: 0.6000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6898 - accuracy: 0.6000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6889 - accuracy: 0.6000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6880 - accuracy: 0.7000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6871 - accuracy: 0.8000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6862 - accuracy: 0.9000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6853 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6844 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6835 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6825 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6816 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6807 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6798 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6789 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6779 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6770 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6760 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6751 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6741 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6731 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6721 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6711 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6701 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6691 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6681 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6670 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6659 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6649 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6638 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6627 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6615 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6604 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6593 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6581 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6569 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6557 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6545 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6533 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6520 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6508 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6495 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6482 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6469 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6456 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6442 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6428 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffaf8120fa0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = paddedCorpus, y = labels, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de646ea9-4dbf-4c20-9944-b3a8f37327ff",
   "metadata": {},
   "source": [
    "## **Evaluating the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a237c827-1d0d-467f-95fa-b15aa4d4d9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 272ms/step - loss: 0.6415 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x = paddedCorpus, y = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bc1b270-6fb7-4acc-94c8-8423679c39c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0\n",
      "Loss: 0.6414653062820435\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy*100}')\n",
    "print(f'Loss: {loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
